{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = pd.read_csv('train_.csv')\n",
    "test_data = pd.read_csv('test_.csv')\n",
    "data = train_data.append(test_data, ignore_index=True)\n",
    "data['tweet'] = data['tweet'].str.replace(\"[^a-zA-Z#]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the stop words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "data['tweet'] = data['tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "tokenized = data['tweet'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokenized = tokenized.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "\n",
    "for i in range(len(tokenized)):\n",
    "    tokenized[i] = ' '.join(tokenized[i])\n",
    "\n",
    "data['tweet'] = tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hastags_collect(x):\n",
    "    \n",
    "    hashtags = []\n",
    "    \n",
    "    for i in x:\n",
    "        ht = re.findall(r\"#(\\w+)\",i)\n",
    "        hashtags.append(ht)\n",
    "        \n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_positive = hastags_collect(data['tweet'][data['label']==0])\n",
    "\n",
    "ht_negative = hastags_collect(data['tweet'][data['label']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#bow_vec = CountVectorizer()\n",
    "#bow = bow_vec.fit_transform(data['tweet'])\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "# TF-IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.fit_transform(data['tweet'])\n",
    "\n",
    "train_tfidf = tfidf[:7920,:]\n",
    "test_tfidf = tfidf[7920:,:]\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#xtrain_tfidf, xvalid_tfidf, ytrain, yvalid = train_test_split(train_tfidf, train_data['label'], random_state=42, test_size=0.3)\n",
    "#\n",
    "#xtrain_tfidf = train_tfidf[ytrain.index]\n",
    "y = train_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a machine learning model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "reg = LogisticRegression()\n",
    "reg.fit(train_tfidf, y)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "dense_ = train_tfidf.toarray()\n",
    "classifier.fit(dense_, y)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "classifier_svm = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier_svm.fit(train_tfidf, y)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random = RandomForestClassifier()\n",
    "random.fit(train_tfidf,y)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier_multi = MultinomialNB()\n",
    "classifier_multi.fit(train_tfidf, y)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "classifier_multi = GradientBoostingClassifier()\n",
    "classifier_multi.fit(train_tfidf,y)\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#bow_vec = CountVectorizer()\n",
    "#bow_test = bow_vec.fit_transform(test_data['tweet'])\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#tfidf_vectorizer_test = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "## TF-IDF feature matrix\n",
    "#tfidf_test = tfidf_vectorizer_test.fit_transform(test_data['tweet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = reg.predict(test_tfidf)\n",
    "prediction = reg.predict_proba(test_tfidf) # predicting on the validation set\n",
    "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_naive = classifier.predict(dense_)\n",
    "\n",
    "pred_svm = classifier_svm.predict(test_tfidf)\n",
    "\n",
    "pred_rf = random.predict(test_tfidf)\n",
    "\n",
    "pred_multinomial = classifier_multi.predict(test_tfidf)\n",
    "#prediction_int_multi = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "#prediction_int_multi = prediction_int.astype(np.int)\n",
    "\n",
    "#pred_GB = gb.predict(test_tfidf)\n",
    "#pred_multinomial_GB = classifier_multi.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
